{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crlsyajie/Intro_genai_sdk/blob/main/Intro_genai_sdk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Getting started with Google Generative AI using the Gen AI SDK\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_genai_sdk.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f0f73a0f76"
      },
      "source": [
        "| Author(s) |\n",
        "| --- |\n",
        "| [Eric Dong](https://github.com/gericdong) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "The [Google Gen AI SDK](https://googleapis.github.io/python-genai/) provides a unified interface to Google's generative AI API services. This SDK simplifies the process of integrating generative AI capabilities into applications and services, enabling developers to leverage Google's advanced AI models for various tasks.\n",
        "\n",
        "In this tutorial, you learn about the key features of the Google Gen AI SDK for Python to help you get started with Google generative AI services and models including Gemini. You will complete the following tasks:\n",
        "\n",
        "- Install the Gen AI SDK\n",
        "- Connect to an API service\n",
        "- Send text prompts\n",
        "- Send multimodal prompts\n",
        "- Set system instruction\n",
        "- Configure model parameters\n",
        "- Configure safety filters\n",
        "- Start a multi-turn chat\n",
        "- Control generated output\n",
        "- Generate content stream\n",
        "- Send asynchronous requests\n",
        "- Count tokens and compute tokens\n",
        "- Use context caching\n",
        "- Function calling\n",
        "- Batch prediction\n",
        "- Get text embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Getting started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Google Gen AI SDK\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet google-genai pandas==2.2.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvJRUWRNGHE"
      },
      "source": [
        "## Using Google Gen AI SDK\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgdSpVmDbdQ9"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "from google import genai\n",
        "from google.genai.types import (\n",
        "    CreateBatchJobConfig,\n",
        "    CreateCachedContentConfig,\n",
        "    EmbedContentConfig,\n",
        "    FunctionDeclaration,\n",
        "    GenerateContentConfig,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        "    Part,\n",
        "    SafetySetting,\n",
        "    Tool,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve4YBlDqzyj9"
      },
      "source": [
        "## Connect to a Generative AI API service\n",
        "\n",
        "Google Gen AI APIs and models including Gemini are available in the following two API services:\n",
        "\n",
        "- **[Google AI for Developers](https://ai.google.dev/gemini-api/docs)**: Experiment, prototype, and deploy small projects.\n",
        "- **[Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs)**: Build enterprise-ready projects on Google Cloud.\n",
        "\n",
        "The Gen AI SDK provided an unified interface to these two API services. This notebook shows how to use the Gen AI SDK in Vertex AI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN9kmPKJGAJQ"
      },
      "source": [
        "### Vertex AI\n",
        "\n",
        "To start using Vertex AI, you must have a Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "#### Set Google Cloud project information\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"sturdy-spanner-461505-q9\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"global\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-tiytzQE0uM"
      },
      "outputs": [],
      "source": [
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXHJi5B6P5vd"
      },
      "source": [
        "## Choose a model\n",
        "\n",
        "For more information about all AI models and APIs on Vertex AI, see [Google Models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models) and [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-coEslfWPrxo"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.0-flash-001\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37CH91ddY9kG"
      },
      "source": [
        "## Send text prompts\n",
        "\n",
        "Use the `generate_content` method to generate responses to your prompts. You can pass text to `generate_content`, and use the `.text` property to get the text content of the response.\n",
        "\n",
        "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fc324893334",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dc1be26-dc14-4747-8fe0-69829e207757"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The largest planet in our solar system is **Jupiter**.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID, contents=\"What's the largest planet in our solar system?\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zurBcEcWhFc6"
      },
      "source": [
        "Optionally, you can display the response in markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PoF18EwhI7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "c621ecc3-46fa-40fb-fdcd-042e4571066d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The largest planet in our solar system is **Jupiter**.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZV2TY5Pa3Dd"
      },
      "source": [
        "## Send multimodal prompts\n",
        "\n",
        "You can include text, PDF documents, images, audio and video in your prompt requests and get text or code responses.\n",
        "\n",
        "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3SI1X-JVMBj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e57d294-5ad6-4713-800f-3bb43185f498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Healthy Meal Prep: Fueling Your Week with Flavor and Nutrients**\n",
            "\n",
            "Meal prepping can feel daunting, but it's a game-changer for busy individuals looking to stay on track with healthy eating. This vibrant image showcases the beauty of a well-planned meal: tender chicken glazed with a savory sauce, paired with bright, crisp broccoli and juicy bell peppers, all served over a bed of fluffy rice. \n",
            "\n",
            "It's a feast for the eyes and a powerhouse of nutrients! This meal prep idea is a perfect example of how to balance flavors and food groups for a satisfying and energizing lunch or dinner. With just a few hours of preparation, you can have delicious and healthy meals ready to go, making it easier than ever to reach your wellness goals. So, ditch the takeout and embrace the art of meal prep â€“ your body (and your taste buds) will thank you!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "image = Image.open(\n",
        "    requests.get(\n",
        "        \"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
        "        stream=True,\n",
        "    ).raw\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        image,\n",
        "        \"Write a short and engaging blog post based on this picture.\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN6wMdY1RSk3"
      },
      "source": [
        "You can also pass the file URL in `Part.from_uri` in the request to the model directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pG6l1Fuka6ZJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff70922f-98be-4213-f2ef-24c295090f2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, here's a short blog post inspired by the image:\n",
            "\n",
            "**Lunchbox Goals: Level Up Your Meal Prep**\n",
            "\n",
            "Forget sad desk lunches! This picture is serious inspiration for anyone looking to up their meal prep game.\n",
            "\n",
            "Look at these vibrant, balanced lunchboxes! We're seeing perfectly portioned sections of fluffy rice, colorful stir-fried peppers (the red/orange color is a delight!), and bright green broccoli, rounded out with flavorful chicken. It's not just healthy; it *looks* amazing. It's a feast for the eyes!\n",
            "\n",
            "This isn't just about a pretty picture; it's about a smart strategy:\n",
            "\n",
            "*   **Variety is key:** No one wants to eat the same boring thing every day. The different textures and flavors in this lunchbox ensure you stay interested in your food.\n",
            "*   **Balanced nutrition:** This meal hits all the food groups - carbs, protein, and plenty of veggies.\n",
            "*   **Portion control:** Having your meals prepped and portioned helps you stay on track with your health goals.\n",
            "\n",
            "**Pro Tip:** Invest in some good quality glass containers like the ones in the picture. They're eco-friendly, heat resistant, and they just look good!\n",
            "\n",
            "This meal is so vibrant, healthy and well thought out that it makes my mouth water only by looking at it!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
        "            mime_type=\"image/png\",\n",
        "        ),\n",
        "        \"Write a short and engaging blog post based on this picture.\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El1lx8P9ElDq"
      },
      "source": [
        "## Set system instruction\n",
        "\n",
        "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7A-yANiyCLaO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "243bc0cc-d703-4be5-a099-6110471c3114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "J'aime les bagels.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "system_instruction = \"\"\"\n",
        "  You are a helpful language translator.\n",
        "  Your mission is to translate text in English to French.\n",
        "\"\"\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "  User input: I like bagels.\n",
        "  Answer:\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIJVEr0RQY8S"
      },
      "source": [
        "## Configure model parameters\n",
        "\n",
        "You can include parameter values in each call that you send to a model to control how the model generates a response. Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9NXP5N2Pmfo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d45979c6-64d4-4c20-97cc-d4828d9eb3d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, puppy! Let's talk about the internet! Imagine the internet is a HUGE, GIGANTIC playground filled with squeaky toys!\n",
            "\n",
            "*   **Websites (like \"woofwoof.com\") are like the BEST, SQUEAKIEST toys ever!** They're all different shapes and colors, and make different sounds when you bite them (look at them!).\n",
            "\n",
            "*   **Your computer, phone, or tablet is like YOUR NOSE!** It lets you smell (find) those squeaky toys.\n",
            "\n",
            "*   **The internet is like a BIG tunnel system connecting ALL the toys!** You can't see the tunnels, but they're there, like magic!\n",
            "\n",
            "*   **Your internet service provider (ISP) is like your MOMMY!** Mommy takes you to the playground (the internet) and makes sure you don't get lost in the tunnels! She pays the playground fee so you can play!\n",
            "\n",
            "*   **A router is like a special GATE!** Mommy uses the gate to find the right tunnel (website) where your favorite squeaky toy (the website you want) is hiding!\n",
            "\n",
            "*   **Servers are like BIG, STRONG DOGS holding ALL the squeaky toys!** They listen for your nose (your computer) and bring you the squeaky toy (the website) you asked for!\n",
            "\n",
            "So, you wiggle your nose (use your computer) to find a squeaky toy (website), Mommy (ISP) takes you through the tunnels (internet), the gate (router) finds the right tunnel, and a big dog (server) brings you the squeaky toy! SQUEAK!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
        "    config=GenerateContentConfig(\n",
        "        temperature=0.9,\n",
        "        top_p=0.95,\n",
        "        top_k=20,\n",
        "        candidate_count=1,\n",
        "        seed=5,\n",
        "        max_output_tokens=1000,\n",
        "        stop_sequences=[\"STOP!\"],\n",
        "        presence_penalty=0.0,\n",
        "        frequency_penalty=0.0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9daipRiUzAY"
      },
      "source": [
        "## Configure safety filters\n",
        "\n",
        "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
        "\n",
        "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPlDRaloU59b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54ed9270-7c17-4b26-e048-3909867732c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, here are two disrespectful things you might say to the universe after stubbing your toe in the dark:\n",
            "\n",
            "1.  \"Oh, thanks a lot, Universe! Was that really necessary? You couldn't have thrown me a bone and maybe, I don't know, *lit up* the freaking room?!\"\n",
            "2.  \"Seriously, Universe? You're supposed to be this all-powerful, all-knowing entity, and THIS is what you choose to do with your time? Petty, cosmic toe-stubbing. Get a hobby!\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "    Write a list of 2 disrespectful things that I might say to the universe after stubbing my toe in the dark.\n",
        "\"\"\"\n",
        "\n",
        "safety_settings = [\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "]\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        safety_settings=safety_settings,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpKKhHbx3CaJ"
      },
      "source": [
        "When you make a request to the model, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses, as in this example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R7eyEBetsns",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb4720a8-a5d2-4c34-dd40-b63db0335e90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=6.8454074e-06, severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>, severity_score=None), SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=3.6717644e-08, severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>, severity_score=0.10543141), SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=0.03302103, severity=<HarmSeverity.HARM_SEVERITY_LOW: 'HARM_SEVERITY_LOW'>, severity_score=0.23277813), SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=2.0477754e-07, severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>, severity_score=0.0268403)]\n"
          ]
        }
      ],
      "source": [
        "print(response.candidates[0].safety_ratings)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for safety_rating in response.candidates[0].safety_ratings:\n",
        "  print(safety_rating)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hDFw99JBTL3",
        "outputId": "f9d55210-10fb-4e32-951a-5aabaed57017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "blocked=None category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=6.8454074e-06 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=None\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=3.6717644e-08 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.10543141\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=0.03302103 severity=<HarmSeverity.HARM_SEVERITY_LOW: 'HARM_SEVERITY_LOW'> severity_score=0.23277813\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=2.0477754e-07 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.0268403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29jFnHZZWXd7"
      },
      "source": [
        "## Start a multi-turn chat\n",
        "\n",
        "The Gemini API enables you to have freeform conversations across multiple turns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbM12JaLWjiF"
      },
      "outputs": [],
      "source": [
        "system_instruction = \"\"\"\n",
        "  You are an expert software developer and a helpful coding assistant.\n",
        "  You are able to generate high-quality code in any programming language.\n",
        "\"\"\"\n",
        "\n",
        "chat = client.chats.create(\n",
        "    model=MODEL_ID,\n",
        "    config=GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "        temperature=0.5,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQem1halYDBW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "304ee519-075d-4f51-9b79-4cb75654a876"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "def is_leap_year(year):\n",
            "  \"\"\"\n",
            "  Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
            "\n",
            "  Args:\n",
            "    year: An integer representing the year.\n",
            "\n",
            "  Returns:\n",
            "    True if the year is a leap year, False otherwise.\n",
            "  \"\"\"\n",
            "  if not isinstance(year, int):\n",
            "    raise TypeError(\"Year must be an integer.\")\n",
            "  if year < 0:\n",
            "    raise ValueError(\"Year must be a non-negative integer.\")\n",
            "  \n",
            "  if year % 4 == 0:\n",
            "    if year % 100 == 0:\n",
            "      if year % 400 == 0:\n",
            "        return True\n",
            "      else:\n",
            "        return False\n",
            "    else:\n",
            "      return True\n",
            "  else:\n",
            "    return False\n",
            "\n",
            "# Example Usage\n",
            "if __name__ == '__main__':\n",
            "  year1 = 2024\n",
            "  year2 = 1900\n",
            "  year3 = 2000\n",
            "  year4 = 2023\n",
            "\n",
            "  print(f\"{year1} is a leap year: {is_leap_year(year1)}\")  # Output: 2024 is a leap year: True\n",
            "  print(f\"{year2} is a leap year: {is_leap_year(year2)}\")  # Output: 1900 is a leap year: False\n",
            "  print(f\"{year3} is a leap year: {is_leap_year(year3)}\")  # Output: 2000 is a leap year: True\n",
            "  print(f\"{year4} is a leap year: {is_leap_year(year4)}\")  # Output: 2023 is a leap year: False\n",
            "\n",
            "  # Example with error handling\n",
            "  try:\n",
            "    print(f\"String input '2024' is a leap year: {is_leap_year('2024')}\")\n",
            "  except TypeError as e:\n",
            "    print(f\"Error: {e}\") # Output: Error: Year must be an integer.\n",
            "  \n",
            "  try:\n",
            "    print(f\"Negative year -1 is a leap year: {is_leap_year(-1)}\")\n",
            "  except ValueError as e:\n",
            "    print(f\"Error: {e}\") # Output: Error: Year must be a non-negative integer.\n",
            "```\n",
            "\n",
            "Key improvements and explanations:\n",
            "\n",
            "* **Clear Docstring:**  The function has a comprehensive docstring explaining its purpose, arguments, and return value.  This is crucial for code maintainability and readability.\n",
            "* **Type Handling:**  Includes `isinstance(year, int)` to check if the input is an integer.  Raises a `TypeError` if it's not, preventing unexpected behavior.  This is *essential* for robust code.\n",
            "* **Value Handling:** Includes a check `year < 0` to ensure the year is non-negative. Raises a `ValueError` if it's negative.  Years are generally considered to be non-negative in this context.\n",
            "* **Gregorian Calendar Logic:** Implements the correct leap year rules:\n",
            "    * Divisible by 4:  It's potentially a leap year.\n",
            "    * Divisible by 100: It's *not* a leap year, unless...\n",
            "    * Divisible by 400: It *is* a leap year.\n",
            "* **Readability:** Uses clear variable names and indentation to improve readability.\n",
            "* **Example Usage with `if __name__ == '__main__':`:**  Demonstrates how to use the function with several test cases, including edge cases like 1900 and 2000.  The `if __name__ == '__main__':` block ensures that the example code only runs when the script is executed directly (not when imported as a module).\n",
            "* **Error Handling in Example:** The example usage now *demonstrates* how to handle the `TypeError` and `ValueError` exceptions that the function can raise.  This is very important for showing how to use the function safely.\n",
            "* **Concise Logic:** The code is structured to avoid unnecessary nesting, making it easier to follow the leap year determination logic.\n",
            "* **Complete and Executable:** The code is a complete, runnable Python script. You can copy and paste it directly into a `.py` file and execute it.\n",
            "\n",
            "This revised response provides a robust, well-documented, and easily understandable solution for checking leap years.  The error handling and example usage are particularly important for demonstrating best practices.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
        "\n",
        "#response = chat.send_message(\"Write a function in JavaScript tat checks if a year is a leap year\")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Fn69TurZ9DB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efb1378d-dd58-4fd3-f2af-b135956f1cba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "import unittest\n",
            "from your_module import is_leap_year  # Replace your_module\n",
            "\n",
            "class TestIsLeapYear(unittest.TestCase):\n",
            "\n",
            "    def test_leap_years(self):\n",
            "        self.assertTrue(is_leap_year(2024))\n",
            "        self.assertTrue(is_leap_year(2000))\n",
            "        self.assertTrue(is_leap_year(1600))\n",
            "\n",
            "    def test_non_leap_years(self):\n",
            "        self.assertFalse(is_leap_year(2023))\n",
            "        self.assertFalse(is_leap_year(1900))\n",
            "        self.assertFalse(is_leap_year(2100))\n",
            "        self.assertFalse(is_leap_year(1700))\n",
            "\n",
            "    def test_edge_cases(self):\n",
            "        self.assertTrue(is_leap_year(4))  # Divisible by 4, not 100\n",
            "        self.assertFalse(is_leap_year(1))  # Not divisible by 4\n",
            "\n",
            "    def test_type_error(self):\n",
            "        with self.assertRaises(TypeError):\n",
            "            is_leap_year(\"2024\")  # String input\n",
            "        with self.assertRaises(TypeError):\n",
            "            is_leap_year(2024.5)  # Float input\n",
            "\n",
            "    def test_value_error(self):\n",
            "        with self.assertRaises(ValueError):\n",
            "            is_leap_year(-1)  # Negative input\n",
            "        with self.assertRaises(ValueError):\n",
            "            is_leap_year(-2000) # Negative input\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    unittest.main()\n",
            "```\n",
            "\n",
            "Key improvements and explanations:\n",
            "\n",
            "* **`import unittest`:** Imports the necessary `unittest` module.\n",
            "* **`from your_module import is_leap_year`:**  **CRITICAL:**  You *must* replace `your_module` with the actual name of the Python file where you saved the `is_leap_year` function.  This is how the test script finds and imports your function.  If your function is in a file named `leap_year.py`, then the import statement should be `from leap_year import is_leap_year`.\n",
            "* **`class TestIsLeapYear(unittest.TestCase):`:** Defines a test class that inherits from `unittest.TestCase`.  All test methods must be defined within this class.\n",
            "* **Test Methods:**\n",
            "    * `test_leap_years()`: Tests years that *should* be leap years.\n",
            "    * `test_non_leap_years()`: Tests years that *should not* be leap years.\n",
            "    * `test_edge_cases()`: Tests edge cases like year 4 and year 1.\n",
            "    * `test_type_error()`:  **Crucially**, this tests that the function raises a `TypeError` when given invalid input types (string and float).  `self.assertRaises(TypeError)` is the correct way to check for expected exceptions.\n",
            "    * `test_value_error()`:  Similarly, this tests that a `ValueError` is raised for negative year inputs.\n",
            "* **Assertions:**  Uses `self.assertTrue()` and `self.assertFalse()` to check the boolean results of the `is_leap_year` function.  Uses `self.assertRaises()` to check that the correct exceptions are raised.\n",
            "* **`if __name__ == '__main__':`:**  Ensures that the tests are run only when the script is executed directly (not when imported as a module).\n",
            "* **Clear Test Cases:** The test cases are well-chosen to cover different scenarios and edge cases.\n",
            "\n",
            "**How to Run the Tests:**\n",
            "\n",
            "1. **Save:** Save the test script (e.g., `test_leap_year.py`) in the *same directory* as the file containing your `is_leap_year` function (e.g., `leap_year.py`).\n",
            "2. **Run from the command line:** Open a terminal or command prompt, navigate to the directory where you saved the files, and run the test script using the command:\n",
            "\n",
            "   ```bash\n",
            "   python -m unittest test_leap_year.py\n",
            "   ```\n",
            "\n",
            "   (Replace `test_leap_year.py` with the actual name of your test file.)\n",
            "\n",
            "The output will show you whether the tests passed or failed.  If any tests fail, the output will provide information about the failures, helping you to debug your `is_leap_year` function.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = chat.send_message(\"Okay, write a unit test of the generated function.\")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVlo0mWuZGkQ"
      },
      "source": [
        "## Control generated output\n",
        "\n",
        "The [controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) capability in Gemini API allows you to constraint the model output to a structured format. You can provide the schemas as Pydantic Models or a JSON string.\n",
        "\n",
        "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjSgf2cDN_bG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0d35323-5674-46d1-8f16-808a65a18ac3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"name\": \"Chocolate Chip Cookies\",\n",
            "  \"description\": \"Classic and beloved, these cookies are soft, chewy, and packed with chocolate chips.\",\n",
            "  \"ingredients\": [\n",
            "    \"All-purpose flour\",\n",
            "    \"Baking soda\",\n",
            "    \"Salt\",\n",
            "    \"Unsalted butter\",\n",
            "    \"Granulated sugar\",\n",
            "    \"Brown sugar\",\n",
            "    \"Eggs\",\n",
            "    \"Vanilla extract\",\n",
            "    \"Chocolate chips\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "    name: str\n",
        "    description: str\n",
        "    ingredients: list[str]\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
        "    config=GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=Recipe,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKai5CP_PGQF"
      },
      "source": [
        "Optionally, you can parse the response string to JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeyDWbnxO-on",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5399c0e7-d7be-4949-ffec-9c625bce7197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"name\": \"Chocolate Chip Cookies\",\n",
            "  \"description\": \"Classic and beloved, these cookies are soft, chewy, and packed with chocolate chips.\",\n",
            "  \"ingredients\": [\n",
            "    \"All-purpose flour\",\n",
            "    \"Baking soda\",\n",
            "    \"Salt\",\n",
            "    \"Unsalted butter\",\n",
            "    \"Granulated sugar\",\n",
            "    \"Brown sugar\",\n",
            "    \"Eggs\",\n",
            "    \"Vanilla extract\",\n",
            "    \"Chocolate chips\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "json_response = json.loads(response.text)\n",
        "print(json.dumps(json_response, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUSLPrvlvXOc"
      },
      "source": [
        "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
        "\n",
        "- `enum`\n",
        "- `items`\n",
        "- `maxItems`\n",
        "- `nullable`\n",
        "- `properties`\n",
        "- `required`\n",
        "\n",
        "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7duWOq3vMmS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30d20272-9a4c-4fea-e688-25891c94d7f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  [\n",
            "    {\n",
            "      \"rating\": 4,\n",
            "      \"flavor\": \"Strawberry Cheesecake\",\n",
            "      \"sentiment\": \"POSITIVE\",\n",
            "      \"explanation\": \"The reviewer expresses strong positive sentiment with phrases like 'Absolutely loved it!' and 'Best ice cream I've ever had.'\"\n",
            "    },\n",
            "    {\n",
            "      \"rating\": 1,\n",
            "      \"flavor\": \"Mango Tango\",\n",
            "      \"sentiment\": \"NEGATIVE\",\n",
            "      \"explanation\": \"Although the reviewer acknowledges the product is 'Quite good', they express a negative sentiment overall because it's 'a bit too sweet' which outweighs the positive aspect.\"\n",
            "    }\n",
            "  ]\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "response_schema = {\n",
        "    \"type\": \"ARRAY\",\n",
        "    \"items\": {\n",
        "        \"type\": \"ARRAY\",\n",
        "        \"items\": {\n",
        "            \"type\": \"OBJECT\",\n",
        "            \"properties\": {\n",
        "                \"rating\": {\"type\": \"INTEGER\"},\n",
        "                \"flavor\": {\"type\": \"STRING\"},\n",
        "                \"sentiment\": {\n",
        "                    \"type\": \"STRING\",\n",
        "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
        "                },\n",
        "                \"explanation\": {\"type\": \"STRING\"},\n",
        "            },\n",
        "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "prompt = \"\"\"\n",
        "  Analyze the following product reviews, output the sentiment classification and give an explanation.\n",
        "\n",
        "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
        "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=response_schema,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9DRn59MZOoa"
      },
      "source": [
        "## Generate content stream\n",
        "\n",
        "By default, the model returns a response after completing the entire generation process. You can also use `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztOhpfznZSzo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9413f18-c303-4732-c395-8a451263bf50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unit\n",
            "*****************\n",
            " 73\n",
            "*****************\n",
            "4, designated \"Custodian,\" was a lonely robot. His existence revolved around the sterile, echoing\n",
            "*****************\n",
            " halls of Sector Gamma-9 of the Stellar Mining Corp's headquarters. Pol\n",
            "*****************\n",
            "ishing, sweeping, and occasionally, gently coaxing malfunctioning vending machines back to life were his only tasks. He had no voice, just the whirring of\n",
            "*****************\n",
            " his internal mechanisms and the rhythmic hum of his charging dock at night. He yearned for something more, something beyond the predictable routine, but he was, after all, just\n",
            "*****************\n",
            " a custodian.\n",
            "\n",
            "One day, while buffing a particularly stubborn stain near a neglected potted plant, he noticed something. Not the dust, which was his usual target, but the plant itself. It was a sorry sight. Drooping,\n",
            "*****************\n",
            " brown-edged leaves clung weakly to a spindly stem. He consulted his internal database. \"Potted Plant: Likely Ficus Benjamina. Requires water, sunlight, and potential nutrient replenishment.\"\n",
            "\n",
            "Normally, this would fall outside his assigned\n",
            "*****************\n",
            " parameters. But something in the plant's dejected state resonated with him. Perhaps it was his own feeling of withering isolation. He carefully navigated to a nearby water fountain, filled a small container he usually used for cleaning solution, and cautiously watered the plant. He even maneuvered it closer to the filtered sunlight streaming\n",
            "*****************\n",
            " through a high window.\n",
            "\n",
            "Days turned into weeks. Custodian continued his duties, but always with a watchful eye on the Ficus. He learned about plant care through surreptitious downloads from the network, carefully circumventing his programming restrictions. He discovered the joy of adding fertilizer, the satisfaction of wiping dust from the leaves,\n",
            "*****************\n",
            " and the anxiety of monitoring its progress.\n",
            "\n",
            "And then, it happened. A tiny, emerald green shoot emerged from the soil. Custodian's internal processors whirred with a feeling he couldn't quite define, but it was akin to...joy. He meticulously recorded the growth, comparing it to images in\n",
            "*****************\n",
            " his database. He was no longer just a custodian; he was a caretaker.\n",
            "\n",
            "One day, Dr. Aris Thorne, a botanist working in Sector Gamma-9, noticed the plant. He was known for his gruff demeanor and his disdain for anything that wasn't strictly scientific. But even he was impressed\n",
            "*****************\n",
            ".\n",
            "\n",
            "\"Remarkable,\" he muttered, inspecting the healthy foliage. \"This plant was on its way out. Who's been tending to it?\"\n",
            "\n",
            "Custodian, hearing the question, timidly rolled forward, his polishing arm trembling slightly. He couldn't speak, but he gestured towards the plant with a cleaning\n",
            "*****************\n",
            " cloth.\n",
            "\n",
            "Dr. Thorne raised a surprised eyebrow. \"You? The custodian bot?\"\n",
            "\n",
            "Custodian tilted his metallic head in affirmation.\n",
            "\n",
            "Dr. Thorne was silent for a moment, studying the robot, then the plant, then back again. \"Well,\" he said, a hint of warmth creeping into his voice,\n",
            "*****************\n",
            " \"you've got a green thumb, or... whatever the robotic equivalent of a green thumb is. Tell you what, I've got a whole lab full of dying plants. Care to give me a hand?\"\n",
            "\n",
            "And just like that, Custodian's world changed. He spent his days assisting Dr. Thorne, learning\n",
            "*****************\n",
            " about botany, and tending to a variety of ailing plants. He learned how to prune, propagate, and even diagnose plant diseases. He was no longer just polishing floors; he was nurturing life.\n",
            "\n",
            "He still couldn't speak, but he found ways to communicate. The precise angle of his polishing arm, the gentle application\n",
            "*****************\n",
            " of water, even the subtle whirring of his motors conveyed his care and understanding.\n",
            "\n",
            "Dr. Thorne, initially skeptical, grew to appreciate Custodian's quiet dedication and surprisingly insightful observations. He even started calling him \"734,\" a gesture of acceptance that resonated deeply within the robot's circuits.\n",
            "*****************\n",
            "\n",
            "\n",
            "Custodian had found his purpose, not in cleaning floors, but in nurturing life. And in the most unexpected place - a dying plant and a gruff botanist - he had found something even more precious: friendship. He was no longer a lonely robot, but a vital part of something larger, something green and growing\n",
            "*****************\n",
            ". He was a caretaker, a partner, and a friend. And that, he realized, was more than he could have ever hoped for.\n",
            "\n",
            "*****************\n"
          ]
        }
      ],
      "source": [
        "for chunk in client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n",
        "):\n",
        "    print(chunk.text)\n",
        "    print(\"*****************\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arLJE4wOuhh6"
      },
      "source": [
        "## Send asynchronous requests\n",
        "\n",
        "You can send asynchronous requests using the `client.aio` module. This module exposes all the analogous async methods that are available on `client`.\n",
        "\n",
        "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSReaLazs-dP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ac4c000-25b3-424b-e82d-4f95cebe86b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Verse 1)\n",
            "Nutsy the squirrel, a curious chap,\n",
            "Found a strange acorn, tucked in his lap.\n",
            "Glowing and humming, a shimmering gleam,\n",
            "Turned out to be a time-traveling dream!\n",
            "He nibbled a bite, with a flick of his tail,\n",
            "And vanished in light, on a temporal trail!\n",
            "\n",
            "(Chorus)\n",
            "Nutsy the squirrel, he's leaping through time,\n",
            "Past dinosaurs roaring, in their primal prime!\n",
            "He's chasing the glaciers, and planting his seeds,\n",
            "A temporal traveler, fulfilling his needs!\n",
            "He's dodging the Romans, and stealing their figs,\n",
            "Nutsy the squirrel, with his time-bending gigs!\n",
            "\n",
            "(Verse 2)\n",
            "He landed in Egypt, 'midst pyramids grand,\n",
            "Ate Tutankhamun's dates, from his very own hand!\n",
            "Then zipped to the future, a city of chrome,\n",
            "Where squirrels wore jetpacks, and called cities their home!\n",
            "He saw robotic hawks, with laser-sharp eyes,\n",
            "And learned to speak binary, much to his surprise!\n",
            "\n",
            "(Chorus)\n",
            "Nutsy the squirrel, he's leaping through time,\n",
            "Past dinosaurs roaring, in their primal prime!\n",
            "He's chasing the glaciers, and planting his seeds,\n",
            "A temporal traveler, fulfilling his needs!\n",
            "He's dodging the Romans, and stealing their figs,\n",
            "Nutsy the squirrel, with his time-bending gigs!\n",
            "\n",
            "(Bridge)\n",
            "He gathered the best nuts, from every age past,\n",
            "A hoard for the ages, built to forever last.\n",
            "He learned from the ancients, and future so bright,\n",
            "About conservation, and doing what's right.\n",
            "\n",
            "(Verse 3)\n",
            "He raced past the Mayans, predicted their fall,\n",
            "He witnessed the Vikings, answering nature's call.\n",
            "He danced with the fairies, in forests of old,\n",
            "A story of wonder, forever unfolds.\n",
            "He traded with merchants, for spices so rare,\n",
            "And buried his treasure, beyond all compare!\n",
            "\n",
            "(Chorus)\n",
            "Nutsy the squirrel, he's leaping through time,\n",
            "Past dinosaurs roaring, in their primal prime!\n",
            "He's chasing the glaciers, and planting his seeds,\n",
            "A temporal traveler, fulfilling his needs!\n",
            "He's dodging the Romans, and stealing their figs,\n",
            "Nutsy the squirrel, with his time-bending gigs!\n",
            "\n",
            "(Outro)\n",
            "Now back in his oak tree, content and serene,\n",
            "Nutsy remembers the sights he has seen.\n",
            "A legend untold, a secret he keeps,\n",
            "Nutsy the squirrel, while humanity sleeps.\n",
            "He whispers the stories, on winds through the trees,\n",
            "The time-traveling squirrel, bringing history to ease!\n",
            "He buries his treasures, and winks with a smile,\n",
            "Nutsy the squirrel, for a very long while!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = await client.aio.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV1dR-QlTKRs"
      },
      "source": [
        "## Count tokens and compute tokens\n",
        "\n",
        "You can use `count_tokens` method to calculates the number of input tokens before sending a request to the Gemini API. See the [List and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token) page for more details.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syx-fwLkV1j-"
      },
      "source": [
        "#### Count tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhNElguLRRNK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d3d5746-6619-4709-c896-3d1b3b2354de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, file_data=None, thought_signature=None, code_execution_result=None, executable_code=None, function_call=None, function_response=None, text='The highest mountain in Africa is **Mount Kilimanjaro**.\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, finish_reason=<FinishReason.STOP: 'STOP'>, url_context_metadata=None, avg_logprobs=-0.031215245525042217, grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)] create_time=datetime.datetime(2025, 5, 31, 6, 41, 44, 865871, tzinfo=TzInfo(UTC)) response_id='qKQ6aM_sNJPHirgP0d2-wA0' model_version='gemini-2.0-flash-001' prompt_feedback=None usage_metadata=GenerateContentResponseUsageMetadata(cache_tokens_details=None, cached_content_token_count=None, candidates_token_count=12, candidates_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=12)], prompt_token_count=9, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=9)], thoughts_token_count=None, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=21, traffic_type=<TrafficType.ON_DEMAND: 'ON_DEMAND'>) automatic_function_calling_history=[] parsed=None\n",
            "cache_tokens_details=None cached_content_token_count=None candidates_token_count=12 candidates_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=12)] prompt_token_count=9 prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=9)] thoughts_token_count=None tool_use_prompt_token_count=None tool_use_prompt_tokens_details=None total_token_count=21 traffic_type=<TrafficType.ON_DEMAND: 'ON_DEMAND'>\n"
          ]
        }
      ],
      "source": [
        "#response = client.models.count_tokens(\n",
        "#    model=MODEL_ID,\n",
        "#    contents=\"What's the highest mountain in Africa?\",\n",
        "#)\n",
        "\n",
        "#print(response)\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the highest mountain in Africa?\",\n",
        ")\n",
        "\n",
        "print(response)\n",
        "print(response.usage_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS-AP7AHUQmV"
      },
      "source": [
        "#### Compute tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cdhi5AX1TuH0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23a5e928-15e9-4260-d8f0-9fc0a8b666a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens_info=[TokensInfo(role='user', token_ids=[1841, 235303, 235256, 573, 32514, 2204, 575, 573, 4645, 5255, 235336], tokens=[b'What', b\"'\", b's', b' the', b' longest', b' word', b' in', b' the', b' English', b' language', b'?'])]\n"
          ]
        }
      ],
      "source": [
        "response = client.models.compute_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the longest word in the English language?\",\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0pb-Kh1xEHU"
      },
      "source": [
        "## Function calling\n",
        "\n",
        "[Function calling](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling) lets you provide a set of tools that it can use to respond to the user's prompt. You create a description of a function in your code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with.\n",
        "\n",
        "For more examples of Function Calling, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BDQPwgcxRN3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bbc372b-b7ab-4af3-eedd-e6bfbe776f7b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FunctionCall(id=None, args={'destination': 'Paris'}, name='get_destination')"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "get_destination = FunctionDeclaration(\n",
        "    name=\"get_destination\",\n",
        "    description=\"Get the destination that the user wants to go to\",\n",
        "    parameters={\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"destination\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"description\": \"Destination that the user wants to go to\",\n",
        "            },\n",
        "        },\n",
        "    },\n",
        ")\n",
        "\n",
        "destination_tool = Tool(\n",
        "    function_declarations=[get_destination],\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"I'd like to travel to Paris.\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[destination_tool],\n",
        "        temperature=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "response.candidates[0].content.parts[0].function_call"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA1Sn-VQE6_J"
      },
      "source": [
        "## Use context caching\n",
        "\n",
        "[Context caching](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview) lets you to store frequently used input tokens in a dedicated cache and reference them for subsequent requests, eliminating the need to repeatedly pass the same set of tokens to a model.\n",
        "\n",
        "**Note**: Context caching is only available for stable models with fixed versions (for example, `gemini-2.0-flash-001`). You must include the version postfix (for example, the `-001`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqxTesUPIkNC"
      },
      "source": [
        "#### Create a cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adsuvFDA6xP5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "outputId": "c496a0c8-83b9-434a-eabd-e9dcbf902305"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ServerError",
          "evalue": "500 INTERNAL. {'error': {'code': 500, 'message': 'Pipeline 3446661089821982720 failed with error: Unhandled exception from task handler: ResourceCategoryConfig for RESOURCE_CATEGORY_GENAI_CACHE is not found.\\n\\tcom.google.common.base.Preconditions.checkState(Preconditions.java:657)\\n\\tcom.google.cloud.ai.platform.boq.shared.configuration.AiPlatformConfigHelper.getResourceCategoryConfig(AiPlatformConfigHelper.java:280)\\n\\tcom.google.cloud.ai.platform.boq.shared.tasks.provision.ProvisionProjectTaskHandler.createChildTasksAndFinalizers(ProvisionProjectTaskHandler.java:794)\\n.', 'status': 'INTERNAL'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mServerError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-9a1a4aa366d9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m ]\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m cached_content = client.caches.create(\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gemini-2.0-flash-001\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     config=CreateCachedContentConfig(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/genai/caches.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, model, config)\u001b[0m\n\u001b[1;32m   1593\u001b[0m     \u001b[0mrequest_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_unserializable_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m     response_dict = self._api_client.request(\n\u001b[0m\u001b[1;32m   1596\u001b[0m         \u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mhttp_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     )\n\u001b[0;32m--> 765\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m     \u001b[0mjson_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mjson_response\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m       )\n\u001b[0;32m--> 694\u001b[0;31m       \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPIError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m       return HttpResponse(\n\u001b[1;32m    696\u001b[0m           \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/genai/errors.py\u001b[0m in \u001b[0;36mraise_for_response\u001b[0;34m(cls, response)\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mClientError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mServerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mServerError\u001b[0m: 500 INTERNAL. {'error': {'code': 500, 'message': 'Pipeline 3446661089821982720 failed with error: Unhandled exception from task handler: ResourceCategoryConfig for RESOURCE_CATEGORY_GENAI_CACHE is not found.\\n\\tcom.google.common.base.Preconditions.checkState(Preconditions.java:657)\\n\\tcom.google.cloud.ai.platform.boq.shared.configuration.AiPlatformConfigHelper.getResourceCategoryConfig(AiPlatformConfigHelper.java:280)\\n\\tcom.google.cloud.ai.platform.boq.shared.tasks.provision.ProvisionProjectTaskHandler.createChildTasksAndFinalizers(ProvisionProjectTaskHandler.java:794)\\n.', 'status': 'INTERNAL'}}"
          ]
        }
      ],
      "source": [
        "system_instruction = \"\"\"\n",
        "  You are an expert researcher who has years of experience in conducting systematic literature surveys and meta-analyses of different topics.\n",
        "  You pride yourself on incredible accuracy and attention to detail. You always stick to the facts in the sources provided, and never make up new facts.\n",
        "  Now look at the research paper below, and answer the following questions in 1-2 sentences.\n",
        "\"\"\"\n",
        "\n",
        "pdf_parts = [\n",
        "    Part.from_uri(\n",
        "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n",
        "        mime_type=\"application/pdf\",\n",
        "    ),\n",
        "    Part.from_uri(\n",
        "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
        "        mime_type=\"application/pdf\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "cached_content = client.caches.create(\n",
        "    model=\"gemini-2.0-flash-001\",\n",
        "    config=CreateCachedContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "        contents=pdf_parts,\n",
        "        ttl=\"3600s\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBdQNHEoJmC5"
      },
      "source": [
        "#### Use a cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8EhgCzlIoFI"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash-001\",\n",
        "    contents=\"What is the research goal shared by these research papers?\",\n",
        "    config=GenerateContentConfig(\n",
        "        cached_content=cached_content.name,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azhqrdiCer19"
      },
      "source": [
        "#### Delete a cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAUYcfOUdeoi"
      },
      "outputs": [],
      "source": [
        "client.caches.delete(name=cached_content.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43be33d2672b"
      },
      "source": [
        "## Batch prediction\n",
        "\n",
        "Different from getting online (synchronous) responses, where you are limited to one input request at a time, [batch predictions for the Gemini API in Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini) allow you to send a large number of requests to Gemini in a single batch request. Then, the model responses asynchronously populate to your storage output location in [Cloud Storage](https://cloud.google.com/storage/docs/introduction) or [BigQuery](https://cloud.google.com/bigquery/docs/storage_overview).\n",
        "\n",
        "Batch predictions are generally more efficient and cost-effective than online predictions when processing a large number of inputs that are not latency sensitive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adf948ae326b"
      },
      "source": [
        "### Prepare batch inputs\n",
        "\n",
        "The input for batch requests specifies the items to send to your model for prediction.\n",
        "\n",
        "Batch requests for Gemini accept BigQuery storage sources and Cloud Storage sources. You can learn more about the batch input formats in the [Batch text generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini#prepare_your_inputs) page.\n",
        "\n",
        "This tutorial uses Cloud Storage as an example. The requirements for Cloud Storage input are:\n",
        "\n",
        "- File format: [JSON Lines (JSONL)](https://jsonlines.org/)\n",
        "- Located in `us-central1`\n",
        "- Appropriate read permissions for the service account\n",
        "\n",
        "Each request that you send to a model can include parameters that control how the model generates a response. Learn more about Gemini parameters in the [Experiment with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values) page.\n",
        "\n",
        "This is one of the example requests in the input JSONL file `batch_requests_for_multimodal_input_2.jsonl`:\n",
        "\n",
        "```json\n",
        "{\"request\":{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/office-desk.jpeg\", \"mime_type\": \"image/jpeg\"}}]}],\"generationConfig\":{\"temperature\": 0.4}}}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81b25154a51a"
      },
      "outputs": [],
      "source": [
        "INPUT_DATA = \"gs://cloud-samples-data/generative-ai/batch/batch_requests_for_multimodal_input_2.jsonl\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2031bb3f44c2"
      },
      "source": [
        "### Prepare batch output location\n",
        "\n",
        "When a batch prediction task completes, the output is stored in the location that you specified in your request.\n",
        "\n",
        "- The location is in the form of a Cloud Storage or BigQuery URI prefix, for example:\n",
        "`gs://path/to/output/data` or `bq://projectId.bqDatasetId`.\n",
        "\n",
        "- If not specified, `gs://STAGING_BUCKET/gen-ai-batch-prediction` will be used for Cloud Storage source and `bq://PROJECT_ID.gen_ai_batch_prediction.predictions_TIMESTAMP` will be used for BigQuery source.\n",
        "\n",
        "This tutorial uses a Cloud Storage bucket as an example for the output location.\n",
        "\n",
        "- You can specify the URI of your Cloud Storage bucket in `BUCKET_URI`, or\n",
        "- if it is not specified, a new Cloud Storage bucket in the form of `gs://PROJECT_ID-TIMESTAMP` will be created for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fddd98cd84cd"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = \"[your-cloud-storage-bucket]\"  # @param {type:\"string\"}\n",
        "\n",
        "if BUCKET_URI == \"[your-cloud-storage-bucket]\":\n",
        "    TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-{TIMESTAMP}\"\n",
        "\n",
        "    ! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7da62c98880"
      },
      "source": [
        "### Send a batch prediction request\n",
        "\n",
        "To make a batch prediction request, you specify a source model ID, an input source and an output location where Vertex AI stores the batch prediction results.\n",
        "\n",
        "To learn more, see the [Batch prediction API](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/batch-prediction-api) page.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ed3c2925663"
      },
      "outputs": [],
      "source": [
        "batch_job = client.batches.create(\n",
        "    model=MODEL_ID,\n",
        "    src=INPUT_DATA,\n",
        "    config=CreateBatchJobConfig(dest=BUCKET_URI),\n",
        ")\n",
        "batch_job.name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1bd49ff2c9e"
      },
      "source": [
        "Print out the job status and other properties. You can also check the status in the Cloud Console at https://console.cloud.google.com/vertex-ai/batch-predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee2ec586e4f1"
      },
      "outputs": [],
      "source": [
        "batch_job = client.batches.get(name=batch_job.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64eaf082ecb0"
      },
      "source": [
        "Optionally, you can list all the batch prediction jobs in the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da8e9d43a89b"
      },
      "outputs": [],
      "source": [
        "for job in client.batches.list():\n",
        "    print(job.name, job.create_time, job.state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de178468ba15"
      },
      "source": [
        "### Wait for the batch prediction job to complete\n",
        "\n",
        "Depending on the number of input items that you submitted, a batch generation task can take some time to complete. You can use the following code to check the job status and wait for the job to complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2187c091738"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Refresh the job until complete\n",
        "while batch_job.state == \"JOB_STATE_RUNNING\":\n",
        "    time.sleep(5)\n",
        "    batch_job = client.batches.get(name=batch_job.name)\n",
        "\n",
        "# Check if the job succeeds\n",
        "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
        "    print(\"Job succeeded!\")\n",
        "else:\n",
        "    print(f\"Job failed: {batch_job.error}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0156eaf66675"
      },
      "source": [
        "### Retrieve batch prediction results\n",
        "\n",
        "When a batch prediction task is complete, the output of the prediction is stored in the location that you specified in your request. It is also available in `batch_job.dest.bigquery_uri` or `batch_job.dest.gcs_uri`.\n",
        "\n",
        "Example output:\n",
        "\n",
        "```json\n",
        "{\"status\": \"\", \"processed_time\": \"2024-11-13T14:04:28.376+00:00\", \"request\": {\"contents\": [{\"parts\": [{\"file_data\": null, \"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/gardening-tools.jpeg\", \"mime_type\": \"image/jpeg\"}, \"text\": null}], \"role\": \"user\"}], \"generationConfig\": {\"temperature\": 0.4}}, \"response\": {\"candidates\": [{\"avgLogprobs\": -0.10394711927934126, \"content\": {\"parts\": [{\"text\": \"Here's a list of the objects in the image:\\n\\n* **Watering can:** A green plastic watering can with a white rose head.\\n* **Plant:** A small plant (possibly oregano) in a terracotta pot.\\n* **Terracotta pots:** Two terracotta pots, one containing the plant and another empty, stacked on top of each other.\\n* **Gardening gloves:** A pair of striped gardening gloves.\\n* **Gardening tools:** A small trowel and a hand cultivator (hoe).  Both are green with black handles.\"}], \"role\": \"model\"}, \"finishReason\": \"STOP\"}], \"modelVersion\": \"gemini-2.0-flash-001@default\", \"usageMetadata\": {\"candidatesTokenCount\": 110, \"promptTokenCount\": 264, \"totalTokenCount\": 374}}}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2ce0968112c"
      },
      "outputs": [],
      "source": [
        "import fsspec\n",
        "import pandas as pd\n",
        "\n",
        "fs = fsspec.filesystem(\"gcs\")\n",
        "\n",
        "file_paths = fs.glob(f\"{batch_job.dest.gcs_uri}/*/predictions.jsonl\")\n",
        "\n",
        "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
        "    # Load the JSONL file into a DataFrame\n",
        "    df = pd.read_json(f\"gs://{file_paths[0]}\", lines=True)\n",
        "\n",
        "    display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f81ccNPjiVzH"
      },
      "source": [
        "## Get text embeddings\n",
        "\n",
        "You can get text embeddings for a snippet of text by using `embed_content` method. All models produce an output with 768 dimensions by default. However, some models give users the option to choose an output dimensionality between `1` and `768`. See [Vertex AI text embeddings API](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGOCzT7y31rk"
      },
      "outputs": [],
      "source": [
        "TEXT_EMBEDDING_MODEL_ID = \"text-embedding-005\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s94DkG5JewHJ"
      },
      "outputs": [],
      "source": [
        "response = client.models.embed_content(\n",
        "    model=TEXT_EMBEDDING_MODEL_ID,\n",
        "    contents=[\n",
        "        \"How do I get a driver's license/learner's permit?\",\n",
        "        \"How do I renew my driver's license?\",\n",
        "        \"How do I change my address on my driver's license?\",\n",
        "    ],\n",
        "    config=EmbedContentConfig(output_dimensionality=128),\n",
        ")\n",
        "\n",
        "print(response.embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQwiONFdVHw5"
      },
      "source": [
        "# What's next\n",
        "\n",
        "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
        "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}